# -*- coding: utf-8 -*-
"""
Created on Tue Sep  2 14:57:51 2025

@author: ä¸­ç§‘é™¢åœ°ç†æ‰€
"""


'''æ¨¡å‹'''
# -*- coding: utf-8 -*-
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import warnings
# warnings.filterwarnings('ignore')
# from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
# from sklearn.pipeline import Pipeline
# from sklearn.preprocessing import StandardScaler
# from sklearn.feature_selection import SelectKBest, mutual_info_regression
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# from sklearn.linear_model import LinearRegression, Ridge, Lasso
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
# from xgboost import XGBRegressor
# from sklearn.base import clone
# import joblib
# from pathlib import Path

# # ä¸­æ–‡ç»˜å›¾è®¾ç½®ï¼ˆå¯é€‰ï¼‰
# plt.rcParams['font.sans-serif'] = ['SimHei']   # è‹¥æ— é»‘ä½“å¯æ”¹ä¸ºå…¶ä»–ä¸­æ–‡å­—ä½“
# plt.rcParams['axes.unicode_minus'] = False


# path = r"E:\åœ°ä¸Šç”Ÿç‰©é‡\æå–ç‰¹å¾\ç‰¹å¾å€¼.csv"
# datak = pd.read_csv(path, encoding='gbk')
# datak = datak.dropna(how='any').copy()


# y = datak.iloc[:, 0].values          # ç›®æ ‡å˜é‡
# X = datak.iloc[:, 1:].values         # ç‰¹å¾çŸ©é˜µ
# feature_names = datak.columns[1:].tolist()
# target_name = datak.columns[0]

# print("=" * 60)
# print("æ·±åº¦æ•°æ®è¯Šæ–­ä¸åˆ†æ")
# print("=" * 60)
# print(f"æ ·æœ¬æ•°é‡: {len(y)}")
# print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
# print(f"ç›®æ ‡å˜é‡ - {target_name}:")
# print(f"  èŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
# print(f"  å‡å€¼Â±æ ‡å‡†å·®: {y.mean():.4f} Â± {y.std():.4f}")
# print(f"  ä¸­ä½æ•°: {np.median(y):.4f}")
# print(f"  ååº¦: {pd.Series(y).skew():.4f}")
# print(f"  å³°åº¦: {pd.Series(y).kurtosis():.4f}")


# #ç›®æ ‡åˆ†å¸ƒå¯è§†åŒ–

# plt.figure(figsize=(15, 10))

# plt.subplot(2, 3, 1)
# plt.hist(y, bins=20, alpha=0.7, edgecolor='black')
# plt.title('ç›®æ ‡å˜é‡åˆ†å¸ƒ')
# plt.xlabel('å€¼'); plt.ylabel('é¢‘æ•°')

# plt.subplot(2, 3, 2)
# plt.boxplot(y)
# plt.title('ç›®æ ‡å˜é‡ç®±çº¿å›¾')

# plt.subplot(2, 3, 3)
# plt.hist(np.log1p(y), bins=20, alpha=0.7, color='green', edgecolor='black')
# plt.title('å¯¹æ•°å˜æ¢ååˆ†å¸ƒ')
# plt.xlabel('log(1+å€¼)')


# #  ç›¸å…³æ€§åˆ†æ

# correlations = []
# for i in range(X.shape[1]):
#     if np.std(X[:, i]) > 0:
#         corr = np.corrcoef(X[:, i], y)[0, 1]
#         correlations.append((feature_names[i], corr))
# correlations.sort(key=lambda x: abs(x[1]), reverse=True)

# print("\n=== ç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æ’å ===")
# for i, (name, corr) in enumerate(correlations[:10]):
#     print(f"{i+1}. {name}: {corr:.4f}")

# plt.subplot(2, 3, 4)
# corr_values = [c for _, c in correlations[:10]]
# feature_names_top = [n for n, _ in correlations[:10]]
# plt.barh(range(len(corr_values)), corr_values)
# plt.yticks(range(len(corr_values)), feature_names_top)
# plt.title('Top 10 ç‰¹å¾ç›¸å…³æ€§')
# plt.xlabel('ç›¸å…³ç³»æ•°')


# # ç›®æ ‡å¯¹æ•°å˜æ¢
# '''å¯¹ç”Ÿç‰©é‡æ•°æ®è¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œé™ä½åæ€ä¸å¼‚æ–¹å·®æ€§ï¼Œä½¿æ¨¡å‹æ›´æ˜“äºå­¦ä¹ '''
# y_log = np.log1p(y)  


# # è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†ï¼ˆåˆ†å±‚ä¾æ®ï¼šy_logåˆ†ç®±ï¼‰
# def make_bins_for_stratify(y_arr, q=5):
#     labels = pd.qcut(y_arr, q=min(q, max(3, len(y_arr)//10)), labels=False, duplicates='drop')
#     return labels
# y_bins_full = make_bins_for_stratify(y_log, q=5)
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y_log, test_size=0.15, stratify=y_bins_full, random_state=42)
# '''é€šè¿‡å°†ç›®æ ‡å˜é‡åˆ†ç®±ååˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†åœ¨ç›®æ ‡åˆ†å¸ƒä¸Šçš„ä¸€è‡´æ€§ï¼Œé¿å…éšæœºåˆ’åˆ†å¸¦æ¥çš„è¯„ä¼°åå·®'''

# # å±•ç¤ºçš„äº’ä¿¡æ¯æ’åï¼ˆåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ï¼Œé¿å…æ³„æ¼ï¼‰
# '''åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„äº’ä¿¡æ¯ï¼Œæ’åºå±•ç¤ºç‰¹å¾é‡è¦æ€§ã€‚éšååœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œ
#   ä½¿ç”¨æ ‡å‡†åŒ–+äº’ä¿¡æ¯é€‰ç‰¹å¾ çš„Pipelineï¼Œå¹¶åœ¨äº¤å‰éªŒè¯å†…éƒ¨å®Œæˆï¼Œé¿å…ä¿¡æ¯æ³„æ¼'''
# scaler_tmp = StandardScaler()
# X_train_scaled_tmp = scaler_tmp.fit_transform(X_train)
# mi_scores = mutual_info_regression(X_train_scaled_tmp, y_train, random_state=42)
# mi_indices = np.argsort(mi_scores)[::-1]
# print("\n=== åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§æ’åï¼ˆä»…è®­ç»ƒé›†ï¼‰ ===")
# for i in range(min(10, len(feature_names))):
#     print(f"{i+1}. {feature_names[mi_indices[i]]}: {mi_scores[mi_indices[i]]:.4f}")
# # æ„å»º é¢„å¤„ç†+é€‰ç‰¹å¾ Pipelineï¼ˆåœ¨CVå†…æ‰§è¡Œï¼Œé¿å…æ³„æ¼ï¼‰
# top_k = min(10, X.shape[1])
# preprocess = Pipeline([
#     ('scaler', StandardScaler()),
#     ('select', SelectKBest(score_func=mutual_info_regression, k=top_k))
# ])


# # ä¸ºå›å½’æ„é€ åˆ†å±‚CVç´¢å¼•
# from sklearn.model_selection import StratifiedKFold
# def make_stratified_regression_splits(X_, y_, n_splits=5, random_state=42):
#     bins = make_bins_for_stratify(y_, q=min(5, len(y_)//2 if len(y_)>=10 else 3))
#     n_splits_eff = min(n_splits, len(np.unique(bins)))
#     n_splits_eff = max(n_splits_eff, 3)  # è‡³å°‘3æŠ˜
#     skf = StratifiedKFold(n_splits=n_splits_eff, shuffle=True, random_state=random_state)
#     return list(skf.split(X_, bins))

# cv_splits = make_stratified_regression_splits(X_train, y_train, n_splits=5, random_state=42)


# # å¤šç®—æ³•æ¯”è¾ƒï¼ˆCVå‡å€¼-æ–¹å·®ç¨³å¥å‡†åˆ™ï¼‰
# def build_model_pipelines(preprocess_pipe):
#     models = {
#         'Linear Regression': LinearRegression(),
#         'Ridge Regression': Ridge(alpha=1.0),
#         'Lasso Regression': Lasso(alpha=0.1),
#         'Random Forest': RandomForestRegressor(
#             n_estimators=300, min_samples_leaf=2, random_state=42
#         ),
#         'Gradient Boosting': GradientBoostingRegressor(
#             n_estimators=300, random_state=42
#         ),
#         'XGBoost': XGBRegressor(
#             n_estimators=500, max_depth=4, subsample=0.8, colsample_bytree=0.8,
#             reg_alpha=0.0, reg_lambda=1.5, learning_rate=0.05,
#             objective='reg:squarederror', random_state=42
#         )
#     }
#     pipes = {}
#     for name, base in models.items():
#         pipes[name] = Pipeline([
#             ('preprocess', preprocess_pipe),
#             ('model', base)
#         ])
#     return pipes

# def test_different_models(X_tr, y_tr, X_te, y_te, preprocess_pipe, cv_index_splits):
#     from sklearn.base import clone
#     pipes = build_model_pipelines(preprocess_pipe)
#     results = {}
#     print("\n=== ä¸åŒç®—æ³•æ€§èƒ½æ¯”è¾ƒ ===")
#     for name, model in pipes.items():
#         try:
#             cv_scores = cross_val_score(model, X_tr, y_tr, cv=cv_index_splits, scoring='r2', n_jobs=-1)
#             model.fit(X_tr, y_tr)
#             y_pred = model.predict(X_te)
#             test_r2 = r2_score(y_te, y_pred)
#             results[name] = {
#                 'CV_R2_mean': float(np.mean(cv_scores)),
#                 'CV_R2_std': float(np.std(cv_scores)),
#                 'Test_R2': float(test_r2),
#                 'model': clone(model).fit(X_tr, y_tr)
#             }
#             print(f"{name:20s}: CV RÂ² = {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}, Test RÂ² = {test_r2:.3f}")
#         except Exception as e:
#             print(f"{name:20s}: é”™è¯¯ - {str(e)}")
#             results[name] = None
#     return results

# results = test_different_models(X_train, y_train, X_test, y_test, preprocess, cv_splits)

# # é€‰æ‹©æœ€ä½³ç®—æ³•ï¼ˆmean - std æœ€å¤§ï¼‰
# valid_results = {k: v for k, v in results.items() if v is not None}
# if valid_results:
#     def score_fn(res): return res['CV_R2_mean'] - res['CV_R2_std']
#     best_model_name, best_info = max(valid_results.items(), key=lambda kv: score_fn(kv[1]))
#     final_model = best_info['model']  # å·²æ‹ŸåˆPipeline
#     print(f"\né€‰æ‹©æœ€ä½³ç®—æ³•: {best_model_name} (åŸºäºCVæ€§èƒ½ä¸ç¨³å®šæ€§)")
# else:
#     best_model_name = "Random Forest"
#     final_model = Pipeline([
#         ('preprocess', preprocess),
#         ('model', RandomForestRegressor(n_estimators=300, min_samples_leaf=2, random_state=42))
#     ]).fit(X_train, y_train)
#     print("æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹ç»“æœï¼Œä½¿ç”¨é»˜è®¤çš„éšæœºæ£®æ—")


# # åœ¨åŸå°ºåº¦è®¡ç®—æŒ‡æ ‡
# def evaluate_model(y_true_log, y_pred_log, use_log_transform=True):
#     if use_log_transform:
#         y_true = np.expm1(y_true_log)
#         y_pred = np.expm1(y_pred_log)
#     else:
#         y_true = y_true_log
#         y_pred = y_pred_log

#     mse = mean_squared_error(y_true, y_pred)
#     rmse = np.sqrt(mse)
#     mae = mean_absolute_error(y_true, y_pred)
#     r2 = r2_score(y_true, y_pred)

#     mask = y_true > 0.1
#     mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if np.any(mask) else np.nan
#     rrmse = (rmse / np.mean(y_true)) * 100 if np.mean(y_true) != 0 else np.nan
#     return mse, rmse, mae, r2, mape, rrmse

# train_pred_log = final_model.predict(X_train)
# test_pred_log  = final_model.predict(X_test)

# train_metrics = evaluate_model(y_train, train_pred_log, True)
# test_metrics  = evaluate_model(y_test,  test_pred_log,  True)

# print("\n" + "="*60)
# print("æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœ")
# print("="*60)
# print(f'ç®—æ³•: {best_model_name}')
# print('è®­ç»ƒé›†: MSE={:.3f}, RMSE={:.3f}, MAE={:.3f}, RÂ²={:.3f}, MAPE={:.3f}%, RRMSE={:.3f}%'.format(*train_metrics))
# print('æµ‹è¯•é›†: MSE={:.3f}, RMSE={:.3f}, MAE={:.3f}, RÂ²={:.3f}, MAPE={:.3f}%, RRMSE={:.3f}%'.format(*test_metrics))

# # ç‰¹å¾é‡è¦æ€§ï¼ˆæ ‘æ¨¡å‹å¯ç”¨ï¼‰
# def plot_feature_importance_if_available(pipeline_model, feature_names_all):
#     model_step = pipeline_model.named_steps['model']
#     if not hasattr(model_step, 'feature_importances_'):
#         return

#     selector = pipeline_model.named_steps['preprocess'].named_steps['select']
#     support_mask = selector.get_support()
#     selected_names = [name for name, keep in zip(feature_names_all, support_mask) if keep]

#     importances = model_step.feature_importances_
#     indices = np.argsort(importances)[::-1]

#     print("\n=== ç‰¹å¾é‡è¦æ€§æ’å ===")
#     top_n = min(10, len(selected_names))
#     for i in range(top_n):
#         print(f"{i+1}. {selected_names[indices[i]]}: {importances[indices[i]]:.4f}")

#     plt.figure(figsize=(10, 6))
#     plt.barh(range(len(selected_names)), importances[indices])
#     plt.yticks(range(len(selected_names)), [selected_names[i] for i in indices])
#     plt.xlabel('ç‰¹å¾é‡è¦æ€§'); plt.title('ç‰¹å¾é‡è¦æ€§æ’å')
#     plt.tight_layout()
#     plt.show()

# plot_feature_importance_if_available(final_model, feature_names)

# # å¯è§†åŒ–ï¼šé¢„æµ‹-å®é™…ã€æ®‹å·®ã€å­¦ä¹ æ›²çº¿
# plt.figure(figsize=(15, 5))

# y_test_original = np.expm1(y_test)
# test_pred_original = np.expm1(test_pred_log)

# plt.subplot(1, 3, 1)
# plt.scatter(y_test_original, test_pred_original, alpha=0.6)
# max_val = max(y_test_original.max(), test_pred_original.max())
# plt.plot([0, max_val], [0, max_val], 'r--', lw=2)
# plt.xlabel('å®é™…å€¼'); plt.ylabel('é¢„æµ‹å€¼')
# plt.title(f'é¢„æµ‹ vs å®é™… (R2={r2_score(y_test_original, test_pred_original):.3f})')

# plt.subplot(1, 3, 2)
# residuals = y_test_original - test_pred_original
# plt.scatter(test_pred_original, residuals, alpha=0.6)
# plt.axhline(y=0, color='r', linestyle='--')
# plt.xlabel('é¢„æµ‹å€¼'); plt.ylabel('æ®‹å·®')
# plt.title('æ®‹å·®åˆ†æ')

# def build_final_pipeline_by_name(name, preprocess_pipe):
#     if name == "Random Forest":
#         base = RandomForestRegressor(n_estimators=300, min_samples_leaf=2, random_state=42)
#     elif name == "XGBoost":
#         base = XGBRegressor(
#             n_estimators=500, max_depth=4, subsample=0.8, colsample_bytree=0.8,
#             reg_alpha=0.0, reg_lambda=1.5, learning_rate=0.05,
#             objective='reg:squarederror', random_state=42
#         )
#     elif name == "Gradient Boosting":
#         base = GradientBoostingRegressor(n_estimators=300, random_state=42)
#     elif name == "Ridge Regression":
#         base = Ridge(alpha=1.0)
#     elif name == "Lasso Regression":
#         base = Lasso(alpha=0.1)
#     else:
#         base = LinearRegression()
#     return Pipeline([('preprocess', preprocess_pipe), ('model', base)])

# plt.subplot(1, 3, 3)
# train_sizes = np.linspace(0.2, 1.0, 9)
# train_scores = []
# test_scores = []
# for size in train_sizes:
#     n_size = max(5, int(len(X_train) * size))
#     X_subset = X_train[:n_size]
#     y_subset = y_train[:n_size]
#     model_lc = build_final_pipeline_by_name(best_model_name, preprocess)
#     model_lc.fit(X_subset, y_subset)
#     y_pred_train = model_lc.predict(X_subset)
#     y_pred_test  = model_lc.predict(X_test)
#     train_scores.append(r2_score(y_subset, y_pred_train))
#     test_scores.append(r2_score(y_test, y_pred_test))

# plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒé›†')
# plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•é›†')
# plt.xlabel('è®­ç»ƒé›†æ¯”ä¾‹'); plt.ylabel('RÂ²å¾—åˆ†')
# plt.title('å­¦ä¹ æ›²çº¿')
# plt.legend()
# plt.tight_layout()
# plt.show()

# # =========================
# # å»ºè®®è¾“å‡º
# # =========================
# print("\n" + "="*60)
# print("æ”¹è¿›å»ºè®®")
# print("="*60)
# if test_metrics[3] >= 0.7:
#     print("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½")
# elif test_metrics[3] >= 0.6:
#     print("âœ… æ¨¡å‹æ€§èƒ½å¯æ¥å—")
# else:
#     print("âš ï¸ éœ€è¦æå‡æ¨¡å‹æ€§èƒ½:")

# print("\né€šç”¨å»ºè®®:")
# print("1. å¢åŠ æ•°æ®é‡: æ”¶é›†æ›´å¤šæ ·æœ¬æ˜¯æå‡æ€§èƒ½æœ€æœ‰æ•ˆçš„æ–¹æ³•")
# print("2. ç‰¹å¾å·¥ç¨‹: å°è¯•åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾å’Œäº¤äº’ç‰¹å¾ï¼ˆä¾‹å¦‚ NDVIÃ—RGR, SAVIÃ—Redï¼‰")
# print("3. å¼‚å¸¸å€¼å¤„ç†: æ£€æŸ¥å¹¶å¤„ç†æç«¯å€¼çš„å½±å“")
# print("4. é¢†åŸŸçŸ¥è¯†: ç»“åˆä¸“ä¸šçŸ¥è¯†é€‰æ‹©æ›´æœ‰æ„ä¹‰çš„ç‰¹å¾")
# print("5. é›†æˆæ–¹æ³•: å°è¯•æ¨¡å‹é›†æˆæˆ–å †å æ–¹æ³•")

# if test_metrics[0] > train_metrics[0] * 1.5:
#     print("\nç‰¹å®šå»ºè®®:")
#     print("ğŸ“‰ æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®:")
#     print("   - å¢å¼ºæ­£åˆ™åŒ–ï¼ˆä¾‹å¦‚æå‡ min_samples_leaf / é™ä½æ ‘æ·±ï¼‰")
#     print("   - å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼ˆä¾‹å¦‚é™ä½ n_estimators æˆ– max_depthï¼‰")
#     print("   - ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹ï¼ˆå²­/å¥—ç´¢ï¼‰åšå¯¹æ¯”")
#     print("   - å¢åŠ è®­ç»ƒæ•°æ®é‡")

# if pd.Series(y).skew() > 1.0:
#     print("\nğŸ“Š ç›®æ ‡å˜é‡åˆ†å¸ƒé«˜åº¦åæ–œï¼Œå»ºè®®:")
#     print("   - ä½¿ç”¨å¯¹æ•°å˜æ¢ï¼ˆå·²é‡‡ç”¨ï¼‰æˆ–Box-Coxç­‰")
#     print("   - å°è¯•åˆ†ä½æ•°å›å½’")
#     print("   - ä½¿ç”¨å¯¹åæ–œåˆ†å¸ƒé²æ£’çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚MSLEã€åŠ æƒMAPEï¼‰")

# print("="*60)

# # ====== ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸å…ƒæ•°æ®ï¼ˆç”¨äºæ …æ ¼æ¨ç†ï¼‰ ======
# out_dir = Path(r"E:\åœ°ä¸Šç”Ÿç‰©é‡\æå–ç‰¹å¾")
# out_dir.mkdir(parents=True, exist_ok=True)

# artifact = {
#     "pipeline": final_model,          # å·²æ‹Ÿåˆ Pipelineï¼ˆå«é¢„å¤„ç†ä¸ç‰¹å¾é€‰æ‹©ï¼‰
#     "best_model_name": best_model_name,
#     "feature_names": feature_names,   # è®­ç»ƒæ—¶ç‰¹å¾åé¡ºåº
#     "target_log1p": True,             # è®­ç»ƒç›®æ ‡æ˜¯å¦ä½¿ç”¨ log1p
# }
# joblib.dump(artifact, out_dir / "agb_model.joblib")
# print(f"[OK] æ¨¡å‹ä¸å…ƒæ•°æ®å·²ä¿å­˜åˆ° {out_dir/'agb_model.joblib'}")







'''æ¨¡æ‹Ÿ'''
# -*- coding: utf-8 -*-
import os
from pathlib import Path
import numpy as np
import rasterio
from rasterio.windows import Window
from rasterio.enums import Resampling
import joblib
import glob
import re

# ============= ç”¨æˆ·é…ç½®åŒº =============
# 1) æ¨¡å‹æ–‡ä»¶
MODEL_PATH = r"E:\åœ°ä¸Šç”Ÿç‰©é‡\æå–ç‰¹å¾\agb_model.joblib"

# 2) ç‰¹å¾æ …æ ¼è·¯å¾„æ˜ å°„ï¼šé”®å¿…é¡»æ˜¯è®­ç»ƒæ—¶çš„ feature_namesï¼Œå€¼æ˜¯å¯¹åº”çš„ 10m æ …æ ¼è·¯å¾„
# å®šä¹‰æ³¢æ®µé¡ºåº
band_order = ["Blue", "CAI", "CRI", "Green", "NDII", "NDVI", "NIR", "RGR", "RVI", "Red", "SATVI", "SAVI", "SWIR1", "SWIR2", "VARI"]

# è·å–æ‰€æœ‰tifæ–‡ä»¶å¹¶æ’åº
feature_rasters = glob.glob(r"E:\åœ°ä¸Šç”Ÿç‰©é‡\åœ°ä¸Šç”Ÿç‰©é‡\7,8\2508\*.tif")

# åˆ›å»ºæ’åºå‡½æ•°
def sort_by_band_name(file_path):
    filename = file_path.split('\\')[-1].lower()  # è·å–æ–‡ä»¶åå¹¶è½¬ä¸ºå°å†™
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ³¢æ®µåç§°
    for i, band in enumerate(band_order):
        if re.search(rf'\b{band.lower()}\b', filename):
            return i
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ³¢æ®µï¼Œæ”¾åœ¨æœ€å
    return len(band_order)

# æŒ‰ç…§æ³¢æ®µé¡ºåºæ’åº
sorted_raster_paths = sorted(feature_rasters, key=sort_by_band_name)

# åˆ›å»ºç‰¹å¾åç§°åˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„å­—å…¸
FEATURE_RASTERS = {}
for band_name, file_path in zip(band_order, sorted_raster_paths):
    FEATURE_RASTERS[band_name] = file_path

# 3) è¾“å‡ºè·¯å¾„
OUT_TIF = r"E:\åœ°ä¸Šç”Ÿç‰©é‡\åœ°ä¸Šç”Ÿç‰©é‡\7,8\2508.tif"

# 4) åˆ†å—å¤§å°ï¼ˆæ ¹æ®æœºå™¨å†…å­˜é€‚å½“è°ƒæ•´ï¼‰
BLOCK_SIZE = 1024

# 5) å½“æœ‰ NoData æ—¶çš„å¤„ç†ç­–ç•¥ï¼šTrue = ä»»ä¸€ç‰¹å¾ä¸º NoData åˆ™ä¸é¢„æµ‹ï¼ˆè¾“å‡º NoDataï¼‰
STRICT_NODATA = True

# 6) è¾“å‡º NoData å€¼
OUTPUT_NODATA = np.nan
# ====================================


def check_and_order_features(artifact, feature_rasters):
    """ä¿è¯ç‰¹å¾åé½å…¨å¹¶æŒ‰è®­ç»ƒé¡ºåºæ’åº"""
    train_feats = artifact["feature_names"]
    missing = [f for f in train_feats if f not in feature_rasters]
    extra = [f for f in feature_rasters if f not in train_feats]
    
    if missing:
        raise ValueError(f"ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾æ …æ ¼: {missing}")
    if extra:
        print(f"[WARN] å‘ç°è®­ç»ƒé›†ä¸­æœªä½¿ç”¨çš„å¤šä½™ç‰¹å¾ï¼Œå°†å¿½ç•¥: {extra}")
    
    # æŒ‰è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºè¿”å› (ç‰¹å¾å, æ–‡ä»¶è·¯å¾„) åˆ—è¡¨
    ordered = [(f, feature_rasters[f]) for f in train_feats]
    return ordered


def open_rasters(ordered_paths):
    """æ‰“å¼€å…¨éƒ¨ç‰¹å¾æ …æ ¼å¹¶è¿›è¡ŒåŸºæœ¬ä¸€è‡´æ€§æ£€æŸ¥"""
    srcs = [rasterio.open(p) for _, p in ordered_paths]
    ref = srcs[0]
    
    for i, src in enumerate(srcs[1:], start=1):
        if src.crs != ref.crs:
            raise ValueError(f"CRS ä¸ä¸€è‡´: {ordered_paths[0][1]} vs {ordered_paths[i][1]}")
        if src.transform != ref.transform:
            raise ValueError(f"ä»¿å°„å˜æ¢(åˆ†è¾¨ç‡/å¯¹é½)ä¸ä¸€è‡´: {ordered_paths[0][1]} vs {ordered_paths[i][1]}")
        if src.width != ref.width or src.height != ref.height:
            raise ValueError(f"æ …æ ¼å¤§å°ä¸ä¸€è‡´: {ordered_paths[0][1]} vs {ordered_paths[i][1]}")
    
    return srcs


def read_block_as_stack(srcs, window):
    """è¯»å–ä¸€ä¸ª window çš„æ‰€æœ‰æ³¢æ®µï¼ˆç‰¹å¾ï¼‰ï¼Œè¿”å› shape=(rows, cols, n_features) ä»¥åŠ nodata mask"""
    arrays = []
    nodata_masks = []
    
    for src in srcs:
        arr = src.read(1, window=window, resampling=Resampling.nearest)  # (rows, cols)
        
        # å¤„ç†å¯èƒ½çš„ NaN å€¼ï¼Œå°†å…¶è½¬æ¢ä¸º NoData
        if src.nodata is not None:
            arr = np.where(np.isnan(arr), src.nodata, arr)
        
        arrays.append(arr)
        ndv = src.nodata
        if ndv is None:
            nodata_masks.append(np.zeros_like(arr, dtype=bool))
        else:
            nodata_masks.append(arr == ndv)
    
    stack = np.stack(arrays, axis=-1)  # (rows, cols, features)
    nodata_any = np.any(np.stack(nodata_masks, axis=-1), axis=-1)  # (rows, cols)
    
    # é¢å¤–æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ NaN å€¼
    nan_mask = np.any(np.isnan(stack), axis=-1)
    combined_nodata_mask = nodata_any | nan_mask
    
    return stack, combined_nodata_mask


def main():
    # 1) è½½å…¥æ¨¡å‹
    artifact = joblib.load(MODEL_PATH)
    pipeline = artifact["pipeline"]
    use_log1p = artifact.get("target_log1p", True)

    # 2) æ£€æŸ¥å¹¶æŒ‰è®­ç»ƒé¡ºåºæ•´ç†ç‰¹å¾
    ordered_feats = check_and_order_features(artifact, FEATURE_RASTERS)

    # 3) æ‰“å¼€æ …æ ¼å¹¶åšä¸€è‡´æ€§æ£€æŸ¥
    srcs = open_rasters(ordered_feats)
    ref = srcs[0]

    # 4) å‡†å¤‡è¾“å‡ºæ•°æ®é›†
    profile = ref.profile.copy()
    profile.update(
        dtype=rasterio.float32,
        count=1,
        compress="LZW",
        nodata=OUTPUT_NODATA,
        BIGTIFF="IF_SAFER"
    )

    h, w = ref.height, ref.width
    out_path = Path(OUT_TIF)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with rasterio.open(out_path, "w", **profile) as dst:
        for row_off in range(0, h, BLOCK_SIZE):
            n_rows = min(BLOCK_SIZE, h - row_off)
            for col_off in range(0, w, BLOCK_SIZE):
                n_cols = min(BLOCK_SIZE, w - col_off)
                win = Window(col_off, row_off, n_cols, n_rows)

                stack, nodata_any = read_block_as_stack(srcs, win)  # (r, c, f)
                r, c, f = stack.shape

                # æœ‰æ•ˆåƒç´ æ©è†œ
                valid_mask = ~nodata_any if STRICT_NODATA else np.ones((r, c), dtype=bool)

                if not np.any(valid_mask):
                    out_block = np.full((r, c), OUTPUT_NODATA, dtype=np.float32)
                    dst.write(out_block, 1, window=win)
                    continue

                X_block = stack.reshape(-1, f)  # (r*c, f)
                valid_idx = np.where(valid_mask.reshape(-1))[0]

                # ä»…å¯¹æœ‰æ•ˆåƒç´ åšé¢„æµ‹
                preds_log = np.full(X_block.shape[0], np.nan, dtype=np.float32)
                try:
                    preds_log[valid_idx] = pipeline.predict(X_block[valid_idx, :]).astype(np.float32)
                except Exception as e:
                    raise RuntimeError(f"é¢„æµ‹å¤±è´¥ï¼ˆçª—å£ row={row_off}, col={col_off}ï¼‰ï¼š{e}")

                # è¿˜åŸåˆ°åŸå°ºåº¦
                preds = np.expm1(preds_log) if use_log1p else preds_log

                # å†™å‡ºï¼ˆæ— æ•ˆåƒç´ ç½® NoDataï¼‰
                out_block = preds.reshape(r, c)
                out_block[~valid_mask] = OUTPUT_NODATA
                dst.write(out_block.astype(np.float32), 1, window=win)

    # 5) å…³é—­æ•°æ®æº
    for s in srcs:
        s.close()

    print(f"[OK] AGB 10m é¢„æµ‹å®Œæˆï¼š{out_path.resolve()}")


if __name__ == "__main__":
    main()